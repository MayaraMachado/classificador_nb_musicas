{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classificador_naive_bayes_musica.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sentimento",
      "language": "python",
      "name": "sentimento"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qqq3M_vPW2p"
      },
      "source": [
        "## Importando os dados\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DC8V9WPPW2w"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv9XVp4NPW25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "896e39ed-edb8-404f-cf7b-39a5ac26aceb"
      },
      "source": [
        "df = pd.read_csv('beyonce_rihanna.csv', index_col=0)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Nome da Música</th>\n",
              "      <th>link</th>\n",
              "      <th>album</th>\n",
              "      <th>letra</th>\n",
              "      <th>artista</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'03 Bonnie &amp; Clyde</td>\n",
              "      <td>/beyonce/03-bonnie-clyde.html</td>\n",
              "      <td>I Am... Yours: An Intimate Performance at Wynn...</td>\n",
              "      <td>Jay-z Uh-uh-uh You ready b? Let's go get 'em. ...</td>\n",
              "      <td>Beyoncé</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>***Flawless (Feat. Chimamanda Ngozi Adichie)</td>\n",
              "      <td>/beyonce/flawless-feat-chimamanda-ngozi-adichi...</td>\n",
              "      <td>BEYONCÉ</td>\n",
              "      <td>Your challengers are a young group from Housto...</td>\n",
              "      <td>Beyoncé</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***Flawless (Feat. Nicki Minaj)</td>\n",
              "      <td>/beyonce/flawless-feat-nicki-minaj.html</td>\n",
              "      <td>BEYONCÉ [Platinum Edition]</td>\n",
              "      <td>Dum-da-de-da Do, do, do, do, do, do (Coming do...</td>\n",
              "      <td>Beyoncé</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1+1</td>\n",
              "      <td>/beyonce/11.html</td>\n",
              "      <td>BEYONCÉ [Platinum Edition]</td>\n",
              "      <td>If I ain't got nothing I got you If I ain't go...</td>\n",
              "      <td>Beyoncé</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6 Inch (Feat. The Weeknd)</td>\n",
              "      <td>/beyonce/6-inch-feat-the-weeknd.html</td>\n",
              "      <td>LEMONADE</td>\n",
              "      <td>Six inch heels She walked in the club like nob...</td>\n",
              "      <td>Beyoncé</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 Nome da Música  ...  artista\n",
              "0                            '03 Bonnie & Clyde  ...  Beyoncé\n",
              "1  ***Flawless (Feat. Chimamanda Ngozi Adichie)  ...  Beyoncé\n",
              "2               ***Flawless (Feat. Nicki Minaj)  ...  Beyoncé\n",
              "3                                           1+1  ...  Beyoncé\n",
              "4                     6 Inch (Feat. The Weeknd)  ...  Beyoncé\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a63DRicqPW3G"
      },
      "source": [
        "## Pré-processamentos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poFLBHOcPW3J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "8c17b443-a444-4473-9e3f-4e74fa0cd790"
      },
      "source": [
        "# Utilizando uma música como exemplo\n",
        "exemplo = df['letra'][10]\n",
        "exemplo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Here I am Looking in the mirror An open face, the pain erased Now the sky is clearer I can see the sun Now that all is, all is said and done, oh  There you are Always strong when I need you You let me give And now I live, fearless and protected With the one I will love After all is, all is said and done  I once believed that hearts were made to bleed (Inside I once believed that hearts were made to bleed, oh baby) But now I'm not afraid to say I need you, I need you so stay with me  These precious (precious) hours (yeah) Greet each dawn in open arms And dream, into tomorrow  Where there's only love After all is, all is said and done  (Yeah baby) Oh baby (Inside I once believed, That hearts were meant to bleed)  (I'll never) I'll never be afraid to say I need you, I need you, so here  Here we are in the still of this moment Fear is gone, hope lives on  We found our happy ending For there's only love (only love) And this sweet, sweet love After all is, all is said and done  Yeah baby after all is (all is)  All is said and done\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY_Ku-a_PW3b"
      },
      "source": [
        "### Tokenização \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcXQSNTOPW3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9bb8b5-3425-4314-aaba-13d5507f18a7"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgTkezQEPW3n"
      },
      "source": [
        "#Tokenizando a primeira música\n",
        "tokens = word_tokenize(exemplo)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iX6icQ4PW34"
      },
      "source": [
        "### Selecionando apenas as letras e deixando todas em minúsculas\n",
        "\n",
        "Para a máquina, pontuações não são necessárias, por isso um pré-processamento necessário é selecionar apenas as letras de um texto. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWfHh-yfPW38"
      },
      "source": [
        "import re\n",
        "letras = re.findall(r'\\b[A-zÀ-úü]+\\b', exemplo.lower())\n",
        "letras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn-sfEMSPW4a"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "Stopwords são palavras que, apesar de muito frequentes, não são importantes/relevantes para a máquina. Entre elas, podemos encontrar artigos como “o” e “uma”, ou preposições como “de” e “em”, entre outras palavras frequentes no idioma. Para removê-las do texto, utilizamos uma lista de stopwords disponível na biblioteca NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RngDaWfjPW4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb68487-e1e5-4929-ea5d-110f750a14b9"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stops = stopwords.words('english')\n",
        "len(stops) #lista de stopwords em inglês "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOwQ1ed6PW4n"
      },
      "source": [
        "Como remover stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWI0dceNPW4p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6f867e22-2c16-4c06-c5e5-b3a77f875a14"
      },
      "source": [
        "sem_stopwords = [palavra for palavra in letras if palavra not in stops] #comparar eficiência comparando com Conjuntos\n",
        "palavras_importantes = \" \".join(sem_stopwords)\n",
        "palavras_importantes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'looking mirror open face pain erased sky clearer see sun said done oh always strong need let give live fearless protected one love said done believed hearts made bleed inside believed hearts made bleed oh baby afraid say need need stay precious precious hours yeah greet dawn open arms dream tomorrow love said done yeah baby oh baby inside believed hearts meant bleed never never afraid say need need still moment fear gone hope lives found happy ending love love sweet sweet love said done yeah baby said done'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afGEg33WPW4z"
      },
      "source": [
        "### Lematização \n",
        "\n",
        "Assim como Stopwords, ter verbos conjugados em um texto não faz diferença quando a máquina vai processá-lo. Por isso, existem duas ferramentas chamadas Lemmatização e Stemmatização. Ambas fazem a mesma coisa: Quando passado um texto como argumento, elas reduzem todas as formas verbais conjugadas à sua raiz. A única diferença, entretanto, é que a função que lemmatiza seu texto reduz todos os verbos a forma verdadeira da raiz  -  por isso quanto maior seu texto, mais tempo essa função demora para rodar no código - , enquanto a função que stemmatiza apenas \"corta\" as palavras no meio usando a raiz como base, o que pode gerar palavras que não existem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3TRNhTTTwdZ"
      },
      "source": [
        "!python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGXq4zZiPW40"
      },
      "source": [
        "import spacy\n",
        "spc = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4b0LRpko5eh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5314337a-5d7a-4dee-9167-9fc58bcc2d04"
      },
      "source": [
        "spc_letras = spc(palavras_importantes)\n",
        "print(spc_letras.to_json()['tokens'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'id': 0, 'start': 0, 'end': 7, 'pos': 'VERB', 'tag': 'VBG', 'dep': 'amod', 'head': 4}, {'id': 1, 'start': 8, 'end': 14, 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'npadvmod', 'head': 2}, {'id': 2, 'start': 15, 'end': 19, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'head': 4}, {'id': 3, 'start': 20, 'end': 24, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'head': 4}, {'id': 4, 'start': 25, 'end': 29, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'head': 5}, {'id': 5, 'start': 30, 'end': 36, 'pos': 'VERB', 'tag': 'VBN', 'dep': 'ROOT', 'head': 5}, {'id': 6, 'start': 37, 'end': 40, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'head': 7}, {'id': 7, 'start': 41, 'end': 48, 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'dobj', 'head': 5}, {'id': 8, 'start': 49, 'end': 52, 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'ROOT', 'head': 8}, {'id': 9, 'start': 53, 'end': 56, 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'head': 10}, {'id': 10, 'start': 57, 'end': 61, 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'head': 8}, {'id': 11, 'start': 62, 'end': 66, 'pos': 'VERB', 'tag': 'VBN', 'dep': 'xcomp', 'head': 10}, {'id': 12, 'start': 67, 'end': 69, 'pos': 'INTJ', 'tag': 'UH', 'dep': 'intj', 'head': 15}, {'id': 13, 'start': 70, 'end': 76, 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'head': 14}, {'id': 14, 'start': 77, 'end': 83, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'head': 15}, {'id': 15, 'start': 84, 'end': 88, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'aux', 'head': 16}, {'id': 16, 'start': 89, 'end': 92, 'pos': 'VERB', 'tag': 'VB', 'dep': 'ccomp', 'head': 10}, {'id': 17, 'start': 93, 'end': 97, 'pos': 'VERB', 'tag': 'VB', 'dep': 'csubj', 'head': 20}, {'id': 18, 'start': 98, 'end': 102, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'head': 19}, {'id': 19, 'start': 103, 'end': 111, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'head': 17}, {'id': 20, 'start': 112, 'end': 121, 'pos': 'VERB', 'tag': 'VBN', 'dep': 'ccomp', 'head': 16}, {'id': 21, 'start': 122, 'end': 125, 'pos': 'NUM', 'tag': 'CD', 'dep': 'nummod', 'head': 22}, {'id': 22, 'start': 126, 'end': 130, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'head': 23}, {'id': 23, 'start': 131, 'end': 135, 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'head': 10}, {'id': 24, 'start': 136, 'end': 140, 'pos': 'VERB', 'tag': 'VBN', 'dep': 'aux', 'head': 25}, {'id': 25, 'start': 141, 'end': 149, 'pos': 'VERB', 'tag': 'VBN', 'dep': 'ccomp', 'head': 23}, {'id': 26, 'start': 150, 'end': 156, 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'head': 27}, {'id': 27, 'start': 157, 'end': 161, 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'head': 25}, {'id': 28, 'start': 162, 'end': 167, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'oprd', 'head': 27}, {'id': 29, 'start': 168, 'end': 174, 'pos': 'ADP', 'tag': 'IN', 'dep': 'advmod', 'head': 27}, {'id': 30, 'start': 175, 'end': 183, 'pos': 'VERB', 'tag': 'VBN', 'dep': 'amod', 'head': 31}, {'id': 31, 'start': 184, 'end': 190, 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'pobj', 'head': 29}, {'id': 32, 'start': 191, 'end': 195, 'pos': 'VERB', 'tag': 'VBD', 'dep': 'acl', 'head': 31}, {'id': 33, 'start': 196, 'end': 201, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'oprd', 'head': 32}, {'id': 34, 'start': 202, 'end': 204, 'pos': 'INTJ', 'tag': 'UH', 'dep': 'intj', 'head': 36}, {'id': 35, 'start': 205, 'end': 209, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'head': 36}, {'id': 36, 'start': 210, 'end': 216, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'nsubj', 'head': 37}, {'id': 37, 'start': 217, 'end': 220, 'pos': 'VERB', 'tag': 'VBP', 'dep': 'ROOT', 'head': 37}, {'id': 38, 'start': 221, 'end': 225, 'pos': 'VERB', 'tag': 'VB', 'dep': 'xcomp', 'head': 37}, {'id': 39, 'start': 226, 'end': 230, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'head': 40}, {'id': 40, 'start': 231, 'end': 235, 'pos': 'VERB', 'tag': 'VB', 'dep': 'ccomp', 'head': 37}, {'id': 41, 'start': 236, 'end': 244, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'head': 43}, {'id': 42, 'start': 245, 'end': 253, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'head': 43}, {'id': 43, 'start': 254, 'end': 259, 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'attr', 'head': 40}, {'id': 44, 'start': 260, 'end': 264, 'pos': 'INTJ', 'tag': 'UH', 'dep': 'ROOT', 'head': 44}, {'id': 45, 'start': 265, 'end': 270, 'pos': 'VERB', 'tag': 'VBP', 'dep': 'compound', 'head': 46}, {'id': 46, 'start': 271, 'end': 275, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'ROOT', 'head': 46}, {'id': 47, 'start': 276, 'end': 280, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'head': 48}, {'id': 48, 'start': 281, 'end': 285, 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'compound', 'head': 49}, {'id': 49, 'start': 286, 'end': 291, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'ROOT', 'head': 49}, {'id': 50, 'start': 292, 'end': 300, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'head': 51}, {'id': 51, 'start': 301, 'end': 305, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'head': 52}, {'id': 52, 'start': 306, 'end': 310, 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ROOT', 'head': 52}, {'id': 53, 'start': 311, 'end': 315, 'pos': 'VERB', 'tag': 'VBN', 'dep': 'xcomp', 'head': 52}, {'id': 54, 'start': 316, 'end': 320, 'pos': 'INTJ', 'tag': 'UH', 'dep': 'ROOT', 'head': 54}, {'id': 55, 'start': 321, 'end': 325, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'ROOT', 'head': 55}, {'id': 56, 'start': 326, 'end': 328, 'pos': 'INTJ', 'tag': 'UH', 'dep': 'intj', 'head': 59}, {'id': 57, 'start': 329, 'end': 333, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'head': 59}, {'id': 58, 'start': 334, 'end': 340, 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'head': 57}, {'id': 59, 'start': 341, 'end': 349, 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ROOT', 'head': 59}, {'id': 60, 'start': 350, 'end': 356, 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'head': 61}, {'id': 61, 'start': 357, 'end': 362, 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'head': 59}, {'id': 62, 'start': 363, 'end': 368, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'head': 65}, {'id': 63, 'start': 369, 'end': 374, 'pos': 'ADV', 'tag': 'RB', 'dep': 'neg', 'head': 65}, {'id': 64, 'start': 375, 'end': 380, 'pos': 'ADV', 'tag': 'RB', 'dep': 'neg', 'head': 65}, {'id': 65, 'start': 381, 'end': 387, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'ccomp', 'head': 61}, {'id': 66, 'start': 388, 'end': 391, 'pos': 'VERB', 'tag': 'VBP', 'dep': 'aux', 'head': 67}, {'id': 67, 'start': 392, 'end': 396, 'pos': 'VERB', 'tag': 'VBP', 'dep': 'ccomp', 'head': 65}, {'id': 68, 'start': 397, 'end': 401, 'pos': 'VERB', 'tag': 'MD', 'dep': 'aux', 'head': 70}, {'id': 69, 'start': 402, 'end': 407, 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'head': 70}, {'id': 70, 'start': 408, 'end': 414, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'ROOT', 'head': 70}, {'id': 71, 'start': 415, 'end': 419, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'head': 70}, {'id': 72, 'start': 420, 'end': 424, 'pos': 'VERB', 'tag': 'VBN', 'dep': 'ROOT', 'head': 72}, {'id': 73, 'start': 425, 'end': 429, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'head': 74}, {'id': 74, 'start': 430, 'end': 435, 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'head': 75}, {'id': 75, 'start': 436, 'end': 441, 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ROOT', 'head': 75}, {'id': 76, 'start': 442, 'end': 447, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'head': 78}, {'id': 77, 'start': 448, 'end': 454, 'pos': 'VERB', 'tag': 'VBG', 'dep': 'compound', 'head': 78}, {'id': 78, 'start': 455, 'end': 459, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'head': 79}, {'id': 79, 'start': 460, 'end': 464, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'ccomp', 'head': 75}, {'id': 80, 'start': 465, 'end': 470, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'head': 82}, {'id': 81, 'start': 471, 'end': 476, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'head': 82}, {'id': 82, 'start': 477, 'end': 481, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'head': 79}, {'id': 83, 'start': 482, 'end': 486, 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ROOT', 'head': 83}, {'id': 84, 'start': 487, 'end': 491, 'pos': 'VERB', 'tag': 'VBN', 'dep': 'xcomp', 'head': 83}, {'id': 85, 'start': 492, 'end': 496, 'pos': 'INTJ', 'tag': 'UH', 'dep': 'ROOT', 'head': 85}, {'id': 86, 'start': 497, 'end': 501, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'head': 87}, {'id': 87, 'start': 502, 'end': 506, 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ROOT', 'head': 87}, {'id': 88, 'start': 507, 'end': 511, 'pos': 'VERB', 'tag': 'VBN', 'dep': 'xcomp', 'head': 87}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSZ772ZqPW5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b7a3ab-1048-43ba-b638-2ba7cc858072"
      },
      "source": [
        "lemmas = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
        "texto_limpo = \" \".join(lemmas)\n",
        "print(texto_limpo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looking mirror open face pain erased sky clearer see sun said done oh always strong need let give live fearless protected one love said done believed hearts made bleed inside believed hearts made bleed oh baby afraid say need need stay precious precious hours yeah greet dawn open arms dream tomorrow love said done yeah baby oh baby inside believed hearts meant bleed never never afraid say need need still moment fear gone hope lives found happy ending love love sweet sweet love said done yeah baby said done\n",
            "look mirror open face pain erase sky clearer see sun say do oh always strong need let give live fearless protect one love say do believe hearts make bleed inside believe hearts make bleed oh baby afraid say need need stay precious precious hours yeah greet dawn open arms dream tomorrow love say do yeah baby oh baby inside believe hearts mean bleed never never afraid say need nee still moment fear go hope lives find happy end love love sweet sweet love say do yeah baby say do\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2yXYKzyTweK"
      },
      "source": [
        "def limpar_texto(texto):\n",
        "    '''\n",
        "    Função para converter todas as letras para sua forma minúscula, selecionar apenas as letras,\n",
        "    remover stopwords e lematizar o texto. \n",
        "    '''\n",
        "    \n",
        "    ### Transforme as letras para minúscula ###\n",
        "    minusculas = texto.lower()\n",
        "    \n",
        "    ### Selecione apenas as letras do texto ##\n",
        "    letras = re.findall(r'\\b[A-zÀ-úü]+\\b', minusculas)\n",
        "    \n",
        "    ### Removendo as stopwords ###\n",
        "    stops = set(stopwords.words('english')) \n",
        "    # Retire as stopwords de letras\n",
        "    palavras_sem_stopwords = [palavra for palavra in letras if palavra not in stops]\n",
        "    # Junte as palavras sem stopwords \n",
        "    palavras_importantes = ' '.join(palavras_sem_stopwords) \n",
        "    \n",
        "    ### Lematização ###\n",
        "    spc_letras = spc(palavras_importantes)\n",
        "    # Lematize o texto \n",
        "    lemmas = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
        "    # Junte os lemmas \n",
        "    texto_limpo = \" \".join(lemmas)\n",
        "    \n",
        "    return texto_limpo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mrrNlaKPW6V"
      },
      "source": [
        "df['Texto Limpo'] = df['letra'].apply(limpar_texto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI9AD4--PW6p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "0b7278ff-0859-44d5-b684-f63f176dcf66"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Nome da Música</th>\n",
              "      <th>link</th>\n",
              "      <th>album</th>\n",
              "      <th>letra</th>\n",
              "      <th>artista</th>\n",
              "      <th>Texto Limpo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'03 Bonnie &amp; Clyde</td>\n",
              "      <td>/beyonce/03-bonnie-clyde.html</td>\n",
              "      <td>I Am... Yours: An Intimate Performance at Wynn...</td>\n",
              "      <td>Jay-z Uh-uh-uh You ready b? Let's go get 'em. ...</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>jay z uh uh uh ready b let go get em look youn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>***Flawless (Feat. Chimamanda Ngozi Adichie)</td>\n",
              "      <td>/beyonce/flawless-feat-chimamanda-ngozi-adichi...</td>\n",
              "      <td>BEYONCÉ</td>\n",
              "      <td>Your challengers are a young group from Housto...</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>challengers young group houston welcome beyonc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***Flawless (Feat. Nicki Minaj)</td>\n",
              "      <td>/beyonce/flawless-feat-nicki-minaj.html</td>\n",
              "      <td>BEYONCÉ [Platinum Edition]</td>\n",
              "      <td>Dum-da-de-da Do, do, do, do, do, do (Coming do...</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>dum da de da come drip candy ground stay yoncé...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1+1</td>\n",
              "      <td>/beyonce/11.html</td>\n",
              "      <td>BEYONCÉ [Platinum Edition]</td>\n",
              "      <td>If I ain't got nothing I got you If I ain't go...</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>get nothing get get something give damn cause ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6 Inch (Feat. The Weeknd)</td>\n",
              "      <td>/beyonce/6-inch-feat-the-weeknd.html</td>\n",
              "      <td>LEMONADE</td>\n",
              "      <td>Six inch heels She walked in the club like nob...</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>six inch heels walk club like nobody business ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 Nome da Música  ...                                        Texto Limpo\n",
              "0                            '03 Bonnie & Clyde  ...  jay z uh uh uh ready b let go get em look youn...\n",
              "1  ***Flawless (Feat. Chimamanda Ngozi Adichie)  ...  challengers young group houston welcome beyonc...\n",
              "2               ***Flawless (Feat. Nicki Minaj)  ...  dum da de da come drip candy ground stay yoncé...\n",
              "3                                           1+1  ...  get nothing get get something give damn cause ...\n",
              "4                     6 Inch (Feat. The Weeknd)  ...  six inch heels walk club like nobody business ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VfSx3kxoheC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "27ec7533-a77f-4724-f97c-0469ca88211c"
      },
      "source": [
        "df[\"Texto Limpo\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'jay z uh uh uh ready b let go get em look young b cruisin west side highway doin like way eyes behind shade necklace reason date blind dates today get thoroughest girl mashin gas grabbin wheel true heart rides wit new bobby whitney time speak durin sex city get carrie feva soon show ova right back souljah cuz mami rida rolla put us togetha gon stop us whateva lack right shoulder track mami keepin focus let lock like suppose bonnie clyde hov b holla chorus need life sin girlfriend girlfriend ride till end boyfriend boyfriend need life sin girlfriend girlfriend ride till end boyfriend boyfriend jay z problem dudes treat one lovin wit respect treat one humpin nothin eva mad somethin oh place comfy atch oh see perfect nobody walkin earths surface girlfriend work kid keep workin hermes birkin bag manolo blahnik timb aviator lens drops mercedes benz time wear burberry swim worry worry anything necessary anything necessary let necessary occur yep chours need life sin girlfriend ride till end boyfriend need life sin girlfriend ride till end boyfriend jay z talk em b beyonce girlfriend somebody hurt even somebody yaa hee sometimes trip happy could put life nobody nothing ever come us promise give life love trust boyfriend jay z one time beyonce put life air breathe believe promise give life love trust boyfirend chours need life sin girlfriend ride till end boyfriend need life sin girlfriend ride till end boyfriend'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o96vg7AZPW61"
      },
      "source": [
        "## Feature Extraction\n",
        "Antes de treinar o nosso modelo, precisamos organizar os nossos documentos em features que o computador consegue entender, assim, vamos precisamos transformar o nosso texto em algum tipo de representação numérica. Para isso, vamos usar o Bag of Words. \n",
        "\n",
        "### Bag of Words \n",
        "**O que é o Bag of Words?:** BoW é uma forma de representação de texto que descreve a ocorrência de palavras em um documento. Para o BoW a ordem não importa, essa forma de representação só se importa se as palavras conhecidas ocorrem ou não no documento (literalmente um \"saco\" de palavras). \n",
        "\n",
        "Para implementarmos o Bag of Words, precisamos de três coisas: \n",
        "1. Um vocabulário com as palavras conhecidas\n",
        "2. A ocorrência dessas palavras\n",
        "3. Formar vetores a partir dos documentos \n",
        "\n",
        "**Exemplo**\n",
        "\n",
        "\"to the left to the left everything you own in the box to the left\"\n",
        "\n",
        "1. Construir o vocabulário\n",
        "\n",
        "    [\"to\", \"the\", \"left\", \"everything\", \"you\", \"own\", \"in\", \"box\"]\n",
        "    \n",
        "\n",
        "2. Ocorrência das palavras\n",
        "\n",
        "    {\"to\": 3, \"the\": 3, \"left\":3, \"everything\":1, \"you\":1, \"own\":1, \"in\":1, \"box\":1}\n",
        "\n",
        "\n",
        "3. Vetores\n",
        "\n",
        "    Considerando que o nosso documento fosse: \"to the left\"\n",
        "\n",
        "    Usando o vocabulário que construímos antes, o nosso vetor seria: \n",
        "\n",
        "    [1, 1, 1, 0, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ggonk5WPW64"
      },
      "source": [
        "### Count Vectorizer \n",
        "Felizmente, temos o CountVectorizer! Com ele, conseguimos implementar todos os passos acima de uma maneira bem simples: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoiGvcEYPW67"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "# Bag of words\n",
        "count_vectorizer = CountVectorizer(binary=True)\n",
        "X = count_vectorizer.fit_transform(df['Texto Limpo'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpwp7cETPW7A"
      },
      "source": [
        "Olhando o nosso vocabulário: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrncvpr2PW7A"
      },
      "source": [
        "count_vectorizer.get_feature_names() #Todas as palavras do nosso vocabulário "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsoJ7CQkPW7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8dd5876-2f9d-4963-a9b8-ce9a15bc732b"
      },
      "source": [
        "count_vectorizer.vocabulary_.get('amo')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "185"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTT5x5WLPW7M"
      },
      "source": [
        "Exemplo da nossa matriz termo-documento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpWLbwtIPW7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "82a29a74-e538-4fd3-a8e5-d6622a86f7f3"
      },
      "source": [
        "df_cv = pd.DataFrame(X.toarray(), columns = count_vectorizer.get_feature_names())\n",
        "df_cv.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aa</th>\n",
              "      <th>aaaaaah</th>\n",
              "      <th>aaah</th>\n",
              "      <th>aah</th>\n",
              "      <th>aahhhh</th>\n",
              "      <th>aaron</th>\n",
              "      <th>abandoning</th>\n",
              "      <th>abanenkani</th>\n",
              "      <th>abaziyo</th>\n",
              "      <th>abit</th>\n",
              "      <th>abita</th>\n",
              "      <th>able</th>\n",
              "      <th>aboard</th>\n",
              "      <th>abrasive</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>abstain</th>\n",
              "      <th>abu</th>\n",
              "      <th>abunch</th>\n",
              "      <th>abuse</th>\n",
              "      <th>acabado</th>\n",
              "      <th>acabo</th>\n",
              "      <th>acabó</th>\n",
              "      <th>acaso</th>\n",
              "      <th>accent</th>\n",
              "      <th>accept</th>\n",
              "      <th>accepte</th>\n",
              "      <th>acceptin</th>\n",
              "      <th>access</th>\n",
              "      <th>accidentally</th>\n",
              "      <th>accomodation</th>\n",
              "      <th>accomplishments</th>\n",
              "      <th>account</th>\n",
              "      <th>accountant</th>\n",
              "      <th>accule</th>\n",
              "      <th>accusations</th>\n",
              "      <th>ace</th>\n",
              "      <th>ache</th>\n",
              "      <th>achetant</th>\n",
              "      <th>achieve</th>\n",
              "      <th>achètera</th>\n",
              "      <th>...</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>yous</th>\n",
              "      <th>youte</th>\n",
              "      <th>youth</th>\n",
              "      <th>youuuuuu</th>\n",
              "      <th>youuuuuuuu</th>\n",
              "      <th>yuh</th>\n",
              "      <th>yum</th>\n",
              "      <th>yummy</th>\n",
              "      <th>yup</th>\n",
              "      <th>yé</th>\n",
              "      <th>zac</th>\n",
              "      <th>zam</th>\n",
              "      <th>zero</th>\n",
              "      <th>zeroes</th>\n",
              "      <th>zinati</th>\n",
              "      <th>ziplock</th>\n",
              "      <th>zo</th>\n",
              "      <th>zonday</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoo</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zung</th>\n",
              "      <th>zz</th>\n",
              "      <th>ángel</th>\n",
              "      <th>ça</th>\n",
              "      <th>échappement</th>\n",
              "      <th>égaré</th>\n",
              "      <th>égarés</th>\n",
              "      <th>égaux</th>\n",
              "      <th>élever</th>\n",
              "      <th>élu</th>\n",
              "      <th>éléverons</th>\n",
              "      <th>état</th>\n",
              "      <th>été</th>\n",
              "      <th>évite</th>\n",
              "      <th>évoque</th>\n",
              "      <th>êt</th>\n",
              "      <th>única</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 7038 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aa  aaaaaah  aaah  aah  aahhhh  aaron  ...  état  été  évite  évoque  êt  única\n",
              "0   0        0     0    0       0      0  ...     0    0      0       0   0      0\n",
              "1   0        0     0    0       0      0  ...     0    0      0       0   0      0\n",
              "2   0        0     0    0       0      0  ...     0    0      0       0   0      0\n",
              "3   0        0     0    0       0      0  ...     0    0      0       0   0      0\n",
              "4   0        0     0    0       0      0  ...     0    0      0       0   0      0\n",
              "\n",
              "[5 rows x 7038 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQasqKkLPW7g"
      },
      "source": [
        "No dataframe acima, cada uma das colunas representa uma das palavras do nosso vocabulário, e cada linha, um dos nossos documentos, ou seja, uma das nossas músicas. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGmN5MILPW8A"
      },
      "source": [
        "## Separando em Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wieWN86PW8C"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = X.toarray()\n",
        "y = df['artista']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V03QPr0QPW8H"
      },
      "source": [
        "## Naive Bayes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJKMuSYLPW8J"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#Criando o Modelo Naive Bayes \n",
        "naive_bayes = MultinomialNB()\n",
        "\n",
        "#.......Treinando o Modelo.......\n",
        "naive_bayes.fit(X_train, y_train)\n",
        "\n",
        "#Fazendo as previsões\n",
        "naive_bayes_pred = naive_bayes.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJNecXsIPW8f"
      },
      "source": [
        "## Métricas \n",
        "Após estarmos com nosso modelo de classificação pronto, devemos avaliá-lo e, para isso, utilizamos as métricas de classificação. \n",
        "### Matriz de confusão\n",
        "Primeiro, quando estamos lidando com um modelo cuja target é categórica (como no nosso caso, em que as músicas pertencem ou a Beyoncé ou a Rihanna), podemos utilizar uma matriz de confusão para analisarmos melhor onde o nosso modelo está acertando e onde ele está errando. Ela apresenta o seguinte formato:\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Fabio_Araujo_Da_Silva/publication/323369673/figure/fig5/AS:597319787479040@1519423543307/Figura-13-Exemplo-de-uma-matriz-de-confusao.png\" alt=\"Exemplo de uma matriz de confusão\"/></a>\n",
        "\n",
        "Na vertical, estão indicados os valores previstos pelo modelo e, na horizontal, os valores reais. Para cada elemento da matriz, temos dois valores associados: o previsto e o real. Se esse valores coincidirem, tem-se uma previsão correta/verdadeira (por exemplo, verdadeiros positivos e verdadeiros negativos, que estão em verde na imagem). Caso contrário, tem-se um erro cometido pelo modelo (como ocorre nos quadrados vermelhos da imagem acima). \n",
        "\n",
        "### Acurácia\n",
        "A acurácia é, basicamente, uma métrica que indica a relação entre quanto o seu modelo acertou do quanto ele avaliou. Considerando a matriz de confusão mostrada, a acurácia seria igual à soma dos verdadeiros positivos com os verdadeiros negativos dividida pelo total (soma dos verdadeiros e falsos positivos e negativos). A acurácia não é uma boa métrica a ser utilizada quando analisamos dados desbalanceados, porque pode acontecer de o modelo prever muito bem o evento mais usual e ser péssimo prevendo o evento raro. Assim, como trata-se de uma média simples de acertos pelo total, a grande quantidade de acertos na previsão do evento mais usual compensaria a baixa taxa de acerto do evento raro, resultando em uma acurácia alta que não reflete corretamente a qualidade de predição do modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi75e7PvPW8g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9a8af3-32e7-4503-c41d-cf3bab42421a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "#Calculando a acurácia\n",
        "acc = accuracy_score(naive_bayes_pred, y_test)\n",
        "\n",
        "#Matriz de confusão \n",
        "cm = confusion_matrix(naive_bayes_pred, y_test)\n",
        "\n",
        "print(\"Acurácia do modelo\", acc)\n",
        "print(\"\\nMatriz de confusão: \\n\", cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia do modelo 0.7788461538461539\n",
            "\n",
            "Matriz de confusão: \n",
            " [[32  3]\n",
            " [20 49]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6mo3KiPPW8w"
      },
      "source": [
        "## Avaliando as músicas\n",
        "Agora, você pode tentar testar o seu modelo com alguma frase e ver a qual cantora ela se assemelha mais: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjPysbIzPW82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468960db-6a40-49b8-8457-365334501d2c"
      },
      "source": [
        "nova_frase = [\"Te amo, te amo\"] \n",
        "teste = count_vectorizer.transform(nova_frase)\n",
        "pred = naive_bayes.predict(teste)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Rihanna']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf7hNU39Twgr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}